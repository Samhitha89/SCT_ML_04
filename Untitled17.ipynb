{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kx1N6Agl3OV",
        "outputId": "bc687491-a9e3-417f-895b-56e022268efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: {'1': 'palm', '2': 'fist', '3': 'thumbs_up', '4': 'peace', '5': 'okay'}\n",
            "Press number key to start/stop recording that gesture. Press 'q' to quit.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, cv2, time, pickle\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "# ========= USER SETTINGS =========\n",
        "MODE = \"collect\"   # choose: \"collect\", \"train\", \"infer\"\n",
        "GESTURES = [\"palm\", \"fist\", \"thumbs_up\", \"peace\", \"okay\"]\n",
        "DATA_DIR = \"gesture_data\"\n",
        "MODEL_FILE = \"gesture_mlp.h5\"\n",
        "ENCODER_FILE = \"label_encoder.pkl\"\n",
        "# =================================\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# (1) DATA COLLECTION\n",
        "# =========================================================\n",
        "def collect_data():\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    for g in GESTURES:\n",
        "        os.makedirs(os.path.join(DATA_DIR, g), exist_ok=True)\n",
        "\n",
        "    mp_hands = mp.solutions.hands\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    with mp_hands.Hands(static_image_mode=False,\n",
        "                        max_num_hands=1,\n",
        "                        min_detection_confidence=0.6,\n",
        "                        min_tracking_confidence=0.6) as hands:\n",
        "        recording = False\n",
        "        label = None\n",
        "        sample_count = {g: len(os.listdir(os.path.join(DATA_DIR,g))) for g in GESTURES}\n",
        "\n",
        "        print(\"Keys:\", {str(i+1):g for i,g in enumerate(GESTURES)})\n",
        "        print(\"Press number key to start/stop recording that gesture. Press 'q' to quit.\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.flip(frame,1)\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            res = hands.process(rgb)\n",
        "\n",
        "            if res.multi_hand_landmarks:\n",
        "                for lm in res.multi_hand_landmarks:\n",
        "                    mp_drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            cv2.putText(frame, f\"Recording: {recording} Label: {label}\", (10,30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if key == ord('q'):\n",
        "                break\n",
        "\n",
        "            if key in [ord(str(i+1)) for i in range(len(GESTURES))]:\n",
        "                idx = int(chr(key)) - 1\n",
        "                label = GESTURES[idx]\n",
        "                recording = not recording\n",
        "                if recording:\n",
        "                    print(f\"Started recording: {label}\")\n",
        "                else:\n",
        "                    print(f\"Stopped recording: {label}. Samples so far: {sample_count[label]}\")\n",
        "\n",
        "            if recording and res.multi_hand_landmarks:\n",
        "                lm = res.multi_hand_landmarks[0]\n",
        "                coords = []\n",
        "                for l in lm.landmark:\n",
        "                    coords.extend([l.x, l.y, l.z])\n",
        "                coords = np.array(coords)\n",
        "                fname = os.path.join(DATA_DIR, label, f\"{label}_{sample_count[label]:05d}.npy\")\n",
        "                np.save(fname, coords)\n",
        "                sample_count[label] += 1\n",
        "\n",
        "            cv2.imshow(\"Collect Gestures\", frame)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# (2) TRAINING\n",
        "# =========================================================\n",
        "def train_model():\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Dropout\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "    X, y = [], []\n",
        "    gestures = sorted(os.listdir(DATA_DIR))\n",
        "    for g in gestures:\n",
        "        folder = os.path.join(DATA_DIR, g)\n",
        "        for file in os.listdir(folder):\n",
        "            if file.endswith(\".npy\"):\n",
        "                v = np.load(os.path.join(folder,file))\n",
        "                X.append(v)\n",
        "                y.append(g)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    print(\"Dataset:\", X.shape, len(set(y)))\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "    y_cat = to_categorical(y_enc)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_cat,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42,\n",
        "                                                        stratify=y_cat)\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(256, input_shape=(X.shape[1],), activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(len(gestures), activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    model.save(MODEL_FILE)\n",
        "    with open(ENCODER_FILE,\"wb\") as f:\n",
        "        pickle.dump(le, f)\n",
        "\n",
        "    print(\"Training complete. Model saved.\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# (3) REALTIME INFERENCE\n",
        "# =========================================================\n",
        "def infer_realtime():\n",
        "    from tensorflow.keras.models import load_model\n",
        "    model = load_model(MODEL_FILE)\n",
        "    le = pickle.load(open(ENCODER_FILE,\"rb\"))\n",
        "\n",
        "    mp_hands = mp.solutions.hands\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    with mp_hands.Hands(static_image_mode=False,\n",
        "                        max_num_hands=1,\n",
        "                        min_detection_confidence=0.6,\n",
        "                        min_tracking_confidence=0.6) as hands:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.flip(frame,1)\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            res = hands.process(rgb)\n",
        "\n",
        "            label_text = \"No hand\"\n",
        "            if res.multi_hand_landmarks:\n",
        "                lm = res.multi_hand_landmarks[0]\n",
        "                coords = []\n",
        "                for l in lm.landmark:\n",
        "                    coords.extend([l.x, l.y, l.z])\n",
        "                coords = np.array(coords).reshape(1,-1)\n",
        "                preds = model.predict(coords, verbose=0)\n",
        "                idx = np.argmax(preds)\n",
        "                label = le.inverse_transform([idx])[0]\n",
        "                prob = preds[0][idx]\n",
        "                label_text = f\"{label} ({prob:.2f})\"\n",
        "                mp_drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            cv2.putText(frame, label_text, (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "            cv2.imshow(\"Realtime Gesture\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# MAIN\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "    if MODE == \"collect\":\n",
        "        collect_data()\n",
        "    elif MODE == \"train\":\n",
        "        train_model()\n",
        "    elif MODE == \"infer\":\n",
        "        infer_realtime()\n",
        "    else:\n",
        "        print(\"Invalid MODE. Choose from: collect, train, infer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29fa3c8a",
        "outputId": "22e6cb6d-d2f3-4b57-f463-5c038dc90603"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    }
  ]
}